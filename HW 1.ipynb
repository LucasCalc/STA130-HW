{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86836514",
   "metadata": {},
   "source": [
    "# HW 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4444995",
   "metadata": {},
   "source": [
    "## 1. Pick one of the datasets from the ChatBot session(s) of the TUT demo (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a0ce4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude               0\n",
      "latitude                0\n",
      "housing_median_age      0\n",
      "total_rooms             0\n",
      "total_bedrooms        207\n",
      "population              0\n",
      "households              0\n",
      "median_income           0\n",
      "median_house_value      0\n",
      "ocean_proximity         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf27282",
   "metadata": {},
   "source": [
    "### Summary of Interaction:\n",
    "\n",
    "In this interaction, I sought assistance in obtaining a CSV dataset with missing values for use in a data analysis task. Initially, I asked for the Bigfoot Sightings dataset, which I believed contained missing data. However, the links provided for that dataset were not functional. The assistant then provided alternative datasets, and we eventually used the California Housing Prices dataset from a reliable GitHub source.\n",
    "\n",
    "After obtaining the dataset, I asked for help in verifying the presence of missing values. The assistant provided a Python code snippet using pandas, which checks for missing values in each column of the dataset. The \"total_bedrooms\" column in the California Housing dataset contains missing values, which makes this dataset suitable for my analysis.\n",
    "\n",
    "The assistant's guidance allowed me to successfully load the dataset and confirm that it has missing values, which fulfills the requirements of my assignment.\n",
    "\n",
    "https://chatgpt.com/share/f0f5c4df-0cbc-4b14-9b50-67ce170442e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09493ab7",
   "metadata": {},
   "source": [
    "## 2. Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a pandas DataFrame has, and then\n",
    "\n",
    "    1. use code provided in your ChatBot session to print out the number of rows and columns of the dataset; and,\n",
    "    2. write your own general definitions of the meaning of \"observations\" and \"variables\" based on asking the ChatBot to explain these terms in the context of your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2fb169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 20640\n",
      "Number of columns: 10\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the DataFrame\n",
    "num_rows, num_columns = df.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387aeb3c",
   "metadata": {},
   "source": [
    "**Observations** - The rows in a data set, typically the objects in the data set.\n",
    "\n",
    "**Variables** - The columns in the data set, being the attributes of the different observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d0e86",
   "metadata": {},
   "source": [
    "### Summary of Interaction:\n",
    "\n",
    "In this session, the user sought assistance with working on a dataset in Python using the pandas library. The dataset in question was accessed via the URL: \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\". The user requested guidance on how to determine and print the number of rows and columns in the dataset.\n",
    "\n",
    "To address this request, the following steps were provided:\n",
    "\n",
    "    Import the pandas library.\n",
    "    Read the dataset from the provided URL using pd.read_csv().\n",
    "    Use the shape attribute of the DataFrame to obtain the number of rows and columns.\n",
    "\n",
    "Sample code was provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd87669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 20640\n",
      "Number of columns: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset from the URL\n",
    "url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Get the shape of the DataFrame\n",
    "num_rows, num_columns = df.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc9aac",
   "metadata": {},
   "source": [
    "Additionally, the user inquired about the definitions of \"observations\" and \"variables\" in the context of their dataset. It was explained that:\n",
    "\n",
    "    Observations refer to the individual rows in the dataset, each representing a single instance or unit of analysis (e.g., a single house in a housing dataset).\n",
    "    Variables refer to the columns in the dataset, each representing a characteristic or attribute of the observations (e.g., number of bedrooms, size of the house).\n",
    "    \n",
    "https://chatgpt.com/share/a642f164-6603-419b-a507-e5bbbd1d5bdc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ddf0f",
   "metadata": {},
   "source": [
    "## 3. Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b526a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
      "0    -122.23     37.88                41.0        880.0           129.0   \n",
      "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
      "2    -122.24     37.85                52.0       1467.0           190.0   \n",
      "3    -122.25     37.85                52.0       1274.0           235.0   \n",
      "4    -122.25     37.85                52.0       1627.0           280.0   \n",
      "\n",
      "   population  households  median_income  median_house_value ocean_proximity  \n",
      "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
      "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
      "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
      "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
      "4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n",
      "\n",
      "Info about the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "None\n",
      "\n",
      "Statistical summary of the dataset:\n",
      "          longitude      latitude  housing_median_age   total_rooms  \\\n",
      "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
      "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
      "std        2.003532      2.135952           12.585558   2181.615252   \n",
      "min     -124.350000     32.540000            1.000000      2.000000   \n",
      "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
      "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
      "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
      "max     -114.310000     41.950000           52.000000  39320.000000   \n",
      "\n",
      "       total_bedrooms    population    households  median_income  \\\n",
      "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
      "mean       537.870553   1425.476744    499.539680       3.870671   \n",
      "std        421.385070   1132.462122    382.329753       1.899822   \n",
      "min          1.000000      3.000000      1.000000       0.499900   \n",
      "25%        296.000000    787.000000    280.000000       2.563400   \n",
      "50%        435.000000   1166.000000    409.000000       3.534800   \n",
      "75%        647.000000   1725.000000    605.000000       4.743250   \n",
      "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
      "\n",
      "       median_house_value  \n",
      "count        20640.000000  \n",
      "mean        206855.816909  \n",
      "std         115395.615874  \n",
      "min          14999.000000  \n",
      "25%         119600.000000  \n",
      "50%         179700.000000  \n",
      "75%         264725.000000  \n",
      "max         500001.000000  \n",
      "\n",
      "Value counts for each column:\n",
      "\n",
      "Column: longitude\n",
      "longitude\n",
      "-118.31    162\n",
      "-118.30    160\n",
      "-118.29    148\n",
      "-118.27    144\n",
      "-118.32    142\n",
      "-118.28    141\n",
      "-118.35    140\n",
      "-118.36    138\n",
      "-118.19    135\n",
      "-118.37    128\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: latitude\n",
      "latitude\n",
      "34.06    244\n",
      "34.05    236\n",
      "34.08    234\n",
      "34.07    231\n",
      "34.04    221\n",
      "34.09    212\n",
      "34.02    208\n",
      "34.10    203\n",
      "34.03    193\n",
      "33.93    181\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: housing_median_age\n",
      "housing_median_age\n",
      "52.0    1273\n",
      "36.0     862\n",
      "35.0     824\n",
      "16.0     771\n",
      "17.0     698\n",
      "34.0     689\n",
      "26.0     619\n",
      "33.0     615\n",
      "18.0     570\n",
      "25.0     566\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: total_rooms\n",
      "total_rooms\n",
      "1527.0    18\n",
      "1613.0    17\n",
      "1582.0    17\n",
      "2127.0    16\n",
      "1717.0    15\n",
      "2053.0    15\n",
      "1607.0    15\n",
      "1722.0    15\n",
      "1471.0    15\n",
      "1703.0    15\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: total_bedrooms\n",
      "total_bedrooms\n",
      "280.0    55\n",
      "331.0    51\n",
      "345.0    50\n",
      "343.0    49\n",
      "393.0    49\n",
      "328.0    48\n",
      "348.0    48\n",
      "394.0    48\n",
      "272.0    47\n",
      "309.0    47\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: population\n",
      "population\n",
      "891.0     25\n",
      "761.0     24\n",
      "1227.0    24\n",
      "1052.0    24\n",
      "850.0     24\n",
      "825.0     23\n",
      "782.0     22\n",
      "999.0     22\n",
      "1005.0    22\n",
      "753.0     21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: households\n",
      "households\n",
      "306.0    57\n",
      "386.0    56\n",
      "335.0    56\n",
      "282.0    55\n",
      "429.0    54\n",
      "375.0    53\n",
      "284.0    51\n",
      "297.0    51\n",
      "278.0    50\n",
      "340.0    50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: median_income\n",
      "median_income\n",
      "3.1250     49\n",
      "15.0001    49\n",
      "2.8750     46\n",
      "2.6250     44\n",
      "4.1250     44\n",
      "3.8750     41\n",
      "3.3750     38\n",
      "3.0000     38\n",
      "4.0000     37\n",
      "3.6250     37\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: median_house_value\n",
      "median_house_value\n",
      "500001.0    965\n",
      "137500.0    122\n",
      "162500.0    117\n",
      "112500.0    103\n",
      "187500.0     93\n",
      "225000.0     92\n",
      "350000.0     79\n",
      "87500.0      78\n",
      "275000.0     65\n",
      "150000.0     64\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: ocean_proximity\n",
      "ocean_proximity\n",
      "<1H OCEAN     9136\n",
      "INLAND        6551\n",
      "NEAR OCEAN    2658\n",
      "NEAR BAY      2290\n",
      "ISLAND           5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Get a summary of the dataset (data types and non-null counts)\n",
    "print(\"\\nInfo about the dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Get statistical summary of the dataset (for numerical columns)\n",
    "print(\"\\nStatistical summary of the dataset:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Show unique value counts for each column\n",
    "print(\"\\nValue counts for each column:\")\n",
    "for column in df.columns:\n",
    "    print(f\"\\nColumn: {column}\")\n",
    "    print(df[column].value_counts().head(10))  # Display top 10 most frequent values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764be8a",
   "metadata": {},
   "source": [
    "### Summary of Interaction:\n",
    "\n",
    "In this discussion, I explored methods to summarize a dataset using Python and the pandas library. I used the housing dataset available at the link https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv.\n",
    "\n",
    "To provide a basic summary of the dataset, I employed functions like:\n",
    "\n",
    "    df.head() to preview the first few rows.\n",
    "    df.info() to obtain information about the dataset's columns, such as data types and non-null counts.\n",
    "    df.describe() to generate a statistical summary for numerical columns, including metrics like mean, standard deviation, and percentiles.\n",
    "    df.isnull().sum() to check for missing values.\n",
    "\n",
    "Additionally, I used df['column'].value_counts() to display the frequency of unique values in each column, which is especially useful for categorical data. A loop was used to provide value counts for each column, displaying the top 10 most frequent values.\n",
    "\n",
    "https://chatgpt.com/share/8db02f12-252e-4cb9-9d9c-730cc1cd7536"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaf7cf",
   "metadata": {},
   "source": [
    "## 4. If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by df.shape and what is reported by df.describe() with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871e0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape:\n",
      "(20640, 10)\n",
      "\n",
      "Description:\n",
      "          longitude      latitude  housing_median_age   total_rooms  \\\n",
      "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
      "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
      "std        2.003532      2.135952           12.585558   2181.615252   \n",
      "min     -124.350000     32.540000            1.000000      2.000000   \n",
      "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
      "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
      "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
      "max     -114.310000     41.950000           52.000000  39320.000000   \n",
      "\n",
      "       total_bedrooms    population    households  median_income  \\\n",
      "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
      "mean       537.870553   1425.476744    499.539680       3.870671   \n",
      "std        421.385070   1132.462122    382.329753       1.899822   \n",
      "min          1.000000      3.000000      1.000000       0.499900   \n",
      "25%        296.000000    787.000000    280.000000       2.563400   \n",
      "50%        435.000000   1166.000000    409.000000       3.534800   \n",
      "75%        647.000000   1725.000000    605.000000       4.743250   \n",
      "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
      "\n",
      "       median_house_value  \n",
      "count        20640.000000  \n",
      "mean        206855.816909  \n",
      "std         115395.615874  \n",
      "min          14999.000000  \n",
      "25%         119600.000000  \n",
      "50%         179700.000000  \n",
      "75%         264725.000000  \n",
      "max         500001.000000  \n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the dataset\n",
    "print(\"\\nShape:\")\n",
    "print(df.shape)  # e.g., (20640, 10) -> 20640 rows, 10 columns\n",
    "\n",
    "# Describe the dataset\n",
    "print(\"\\nDescription:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89399fc3",
   "metadata": {},
   "source": [
    "##### a) The number of columns analyzed:\n",
    "Non-numeric values are not included in the \"df.describe()\" method as it only deals with numeric values. This means that despite df.shape stating that the data set has 10 columns, the df.describe() method only lists 9, as the \"Ocean Proximity\" column in the data set holds a non-numeric data type, and is therefore excluded.\n",
    "##### b) the values it reports in the \"count\" column\n",
    "Though most of this data set does not contain missing values, the \"total bedrooms\" column does have some missing values. This causes a discrepancy between the number of rows stated by the df.shape attribute and the count of values in the total bedrooms column stated by the df.describe() method.\n",
    "\n",
    "### Summary of Interaction\n",
    "\n",
    "In this conversation, I learned how to explain the discrepancies between the output of df.shape and df.describe() when analyzing a dataset in Python using Pandas. The dataset in question contains both numeric and non-numeric variables, as well as missing values in numeric columns.\n",
    "\n",
    "    Non-numeric Columns: The function df.describe() by default only analyzes numeric columns. Non-numeric columns (such as strings or categorical variables) are excluded from the summary statistics, which explains why fewer columns are reported compared to df.shape, which includes all columns.\n",
    "\n",
    "    Missing Values: The \"count\" column in df.describe() shows the number of non-missing (non-NaN) values for each column. If a column has missing values, the count will be lower than the total number of rows (as given by df.shape). This can cause the reported count in df.describe() to be less than the total number of rows in the dataset.\n",
    "\n",
    "By combining these two explanations, I can account for any differences between the overall structure of the dataset and the summary statistics provided by Pandas' df.describe() function.\n",
    "\n",
    "https://chatgpt.com/share/f985a6e6-bf17-408f-a85d-02b90e66dae1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99213b",
   "metadata": {},
   "source": [
    "## 5. Use your ChatBot session to help understand the difference between the following and then provide your own paraphrasing summarization of that difference\n",
    "\n",
    "    - an \"attribute\", such as df.shape which does not end with ()\n",
    "    - and a \"method\", such as df.describe() which does end with ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe668271",
   "metadata": {},
   "source": [
    "**Attributes** - A property/characteristic of an object. Comparable to a variable, however an attribute is \"owned\" or \"contained\" within an object.\n",
    "\n",
    "**Method** - A function that is native to an object and that executes an action on the object.\n",
    "\n",
    "**Key Differences**\n",
    "\n",
    "- Both are \"owned\" by an object, however attributes are like variables, while methods are like functions.\n",
    "\n",
    "- An attribute does not require parentheses to be accessed because it does not have parameters (ex: df.shape). Oppositely, methods are functions that require inputs (or its default parameter), so it must be used with parentheses (ex: df.describe()).\n",
    "    \n",
    "### Summary of Interaction\n",
    "\n",
    "In this conversation, I explored the difference between attributes and methods in Python, particularly when using the pandas library:\n",
    "\n",
    "    Attributes (e.g., df.shape) represent properties of an object and store information about it. They are accessed without parentheses. For instance, df.shape returns the dimensions (rows and columns) of a DataFrame.\n",
    "\n",
    "    Methods (e.g., df.describe()) are functions associated with an object that perform actions or computations. They are called with parentheses. For example, df.describe() computes and returns summary statistics for numerical columns in the DataFrame.\n",
    "\n",
    "The key distinction is that attributes provide stored information directly, while methods execute code to perform tasks and return results.\n",
    "\n",
    "https://chatgpt.com/share/44d177d6-0654-4b68-b754-544da5bac5a5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3562a48",
   "metadata": {},
   "source": [
    "## 6. The df.describe() method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4d323c",
   "metadata": {},
   "source": [
    "**Count** - The number of non-null (non-missing) observations in each column.\n",
    "\n",
    "**Mean** - The arithmetic average of the values in each column. It is calculated as the sum of all the values divided by the count of the values.\n",
    "\n",
    "**Standard Deviation** (std) - A measure of the dispersion or spread of the data. It tells how much the values deviate from the mean. The larger the standard deviation, the more spread out the data is.\n",
    "\n",
    "**Minimum (min)** - The smallest value in each column.\n",
    "\n",
    "**25th Percentile (25%)** - Also known as the first quartile (Q1). This value represents the point below which 25% of the data falls. It gives an idea of the lower bound of the middle 50% of the data.\n",
    "\n",
    "**50th Percentile (50%)** - Also known as the median. This is the middle value of the data set, where 50% of the values are below it and 50% are above it.\n",
    "\n",
    "**75th Percentile (75%)** - Also known as the third quartile (Q3). This value represents the point below which 75% of the data falls. It gives an idea of the upper bound of the middle 50% of the data.\n",
    "\n",
    "**Maximum (max)** - The largest value in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557e21f",
   "metadata": {},
   "source": [
    "### Summary of Interaction\n",
    "\n",
    "In this conversation, the summary statistics generated by the df.describe() method in Python's pandas library were discussed. The method provides the following key metrics:\n",
    "\n",
    "    Count: Number of non-null observations.\n",
    "    Mean: The average value of the data.\n",
    "    Standard Deviation (std): A measure of data spread from the mean.\n",
    "    Minimum (min): The smallest value in the dataset.\n",
    "    25th Percentile (25%): The value below which 25% of the data falls (first quartile).\n",
    "    50th Percentile (50%): The median, or the middle value of the data.\n",
    "    75th Percentile (75%): The value below which 75% of the data falls (third quartile).\n",
    "    Maximum (max): The largest value in the dataset.\n",
    "\n",
    "These statistics provide insights into the distribution, central tendency, and variability of the dataset being analyzed.\n",
    "\n",
    "https://chatgpt.com/share/c50bd834-fed3-41f5-b5d7-992ed779661a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62917bd",
   "metadata": {},
   "source": [
    "## 7. Missing data can be considered \"across rows\" or \"down columns\". Consider how df.dropna() or del df['col'] should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words\n",
    "\n",
    "    1. Provide an example of a \"use case\" in which using df.dropna() might be peferred over using del df['col']\n",
    "    2. Provide an example of \"the opposite use case\" in which using del df['col'] might be preferred over using df.dropna()\n",
    "    3. Discuss why applying del df['col'] before df.dropna() when both are used together could be important\n",
    "    4. Remove all missing data from one of the datasets you're considering using some combination of del df['col'] and/or df.dropna() and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a11ee",
   "metadata": {},
   "source": [
    "**1. Use case favoring df.dropna()**\n",
    "\n",
    "df.dropna() automatically deletes any row (0/default) or column (1) that contains missing data (NaN). Therefore, it is favourable when trying to clean a data set of missing values, especially in the case that there are multiple rows/columns that contain missing values\n",
    "\n",
    "**2. Use case favoring del df['col']**\n",
    "\n",
    "del df['col'] deletes the named column in the square brackets. This is a favourable operation if you are trying to delete a single and specific column for any reason, especially those other than the presence of missing values.\n",
    "\n",
    "**3. Applying del df['col'] before df.dropna()**\n",
    "\n",
    "Applying del df['col'] before df.dropna() when both are used together could be important in the scenario where you are intending on deleting rows with missing values as well as a specific column. If there is a row that is missing a value in the column you intend to drop but not in any other, using df.dropna() first would delete that row for no reason, as it would not have a missing value had del df['col'] been used first. By using del df['col'] before df.dropna() you can avoid this situation and retain that row of data that would have been erased. \n",
    "\n",
    "**4. Remove all missing data from the \"Housing Dataset\"**\n",
    "\n",
    "As learned in previous questions, the only missing data that the data set contains is in the \"total_bedrooms\" column. This column is missing 207 values. Here, I am faced with the decision between using df.dropna() to delete all of the rows that contain a missing value in this column, or to use del df['col'] and delete the column altogether. My intention is to delete the least amount of data as possible, which led me to decide to use df.dropna() to delete the 207 rows that are missing a value. The justification behind this is that by deleting the 207 rows, I am only deleting 2,070 values (207 rows x 10 columns). On the other hand, by using  del df['col'] to erase the \"total_bedrooms\" column would erase 20640 values (20640 rows x 1 column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7a6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before:\n",
      "\n",
      "longitude               0\n",
      "latitude                0\n",
      "housing_median_age      0\n",
      "total_rooms             0\n",
      "total_bedrooms        207\n",
      "population              0\n",
      "households              0\n",
      "median_income           0\n",
      "median_house_value      0\n",
      "ocean_proximity         0\n",
      "dtype: int64\n",
      "Number of rows: 20640\n",
      "Number of columns: 10\n",
      "\n",
      "After:\n",
      "\n",
      "longitude             0\n",
      "latitude              0\n",
      "housing_median_age    0\n",
      "total_rooms           0\n",
      "total_bedrooms        0\n",
      "population            0\n",
      "households            0\n",
      "median_income         0\n",
      "median_house_value    0\n",
      "ocean_proximity       0\n",
      "dtype: int64\n",
      "Number of rows: 20433\n",
      "Number of columns: 10\n"
     ]
    }
   ],
   "source": [
    "# Display the before the deletion\n",
    "print(\"\\nBefore:\\n\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(missing_values)\n",
    "\n",
    "# Get the shape of the DataFrame\n",
    "num_rows, num_columns = df.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "newdf = df.dropna()\n",
    "\n",
    "print(\"\\nAfter:\\n\")\n",
    "\n",
    "# Check for missing values in each column\n",
    "new_missing_values = newdf.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(new_missing_values)\n",
    "\n",
    "# Get the shape of the DataFrame\n",
    "new_num_rows, new_num_columns = newdf.shape\n",
    "\n",
    "# Print the number of rows and columns\n",
    "print(f\"Number of rows: {new_num_rows}\")\n",
    "print(f\"Number of columns: {new_num_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0eb8d6",
   "metadata": {},
   "source": [
    "### Summary of Interaction\n",
    "\n",
    "In this conversation, we explored the difference between del df['col'] and df.dropna() in pandas and why the order of applying them matters.\n",
    "\n",
    "    del df['col'] removes a specific column from the DataFrame permanently.\n",
    "    df.dropna() removes rows or columns containing missing values (NaN).\n",
    "\n",
    "Importance of Order:\n",
    "\n",
    "Applying del df['col'] before df.dropna() is crucial when the column to be deleted contains many missing values. Deleting the column first prevents unnecessary row loss during dropna(), ensuring only relevant data is considered. This approach also improves efficiency and ensures valid rows aren't dropped due to irrelevant columns.\n",
    "\n",
    "https://chatgpt.com/share/d29dbd48-a5d5-46e3-9b0b-96451fbc794d\n",
    "\n",
    "#### Note: I wrote the code here without the chatbot using past pieces of code and my own understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fcf921",
   "metadata": {},
   "source": [
    "## 8. Give brief explanations in your own words for any requested answers to the questions below\n",
    "\n",
    "1. Use your ChatBot session to understand what df.groupby(\"col1\")[\"col2\"].describe() does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you\n",
    "\n",
    "2. Assuming you've not yet removed missing values in the manner of question \"7\" above, df.describe() would have different values in the count value for different data columns depending on the missingness present in the original data. Why do these capture something fundamentally different from the values in the count that result from doing something like df.groupby(\"col1\")[\"col2\"].describe()?\n",
    "\n",
    "3. Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors: first share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT\n",
    "\n",
    "    - Forget to include import pandas as pd in your code. Use Kernel->Restart from the notebook menu to restart the jupyter notebook session unload imported libraries and start over so you can create this error. When python has an error, it sometimes provides a lot of \"stack trace\" output, but that's not usually very important for troubleshooting. For this problem for example, all you need to share with ChatGPT or search on google is \"NameError: name 'pd' is not defined\"\n",
    "\n",
    "    - Mistype \"titanic.csv\" as \"titanics.csv\"\n",
    "        If ChatBot troubleshooting is based on downloading the file, just replace the whole url with \"titanics.csv\" and try to troubleshoot the subsequent FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv' (assuming the file is indeed not present). Explore introducing typos into a couple other parts of the url and note the slightly different errors this produces\n",
    "\n",
    "    - Try to use a dataframe before it's been assigned into the variable\n",
    "        You can simulate this by just misnaming the variable. For example, if you should write df.groupby(\"col1\")[\"col2\"].describe() based on how you loaded the data, then instead write DF.groupby(\"col1\")[\"col2\"].describe(). Make sure you've fixed your file name so that's not the error any more\n",
    "\n",
    "    - Forget one of the parentheses somewhere the code\n",
    "        For example, if the code should be pd.read_csv(url) the change it to pd.read_csv(url\n",
    "\n",
    "     - Mistype one of the names of the chained functions with the code. For example, try something like df.group_by(\"col1\")[\"col2\"].describe() and df.groupby(\"col1\")[\"col2\"].describle()\n",
    "\n",
    "     - Use a column name that's not in your data for the groupby and column selection. For example, try capitalizing the columns for example replacing \"sex\" with \"Sex\" in titanic_df.groupby(\"sex\")[\"age\"].describe(), and then instead introducing the same error of \"age\"\n",
    "\n",
    "      - Forget to put the column name as a string in quotes for the groupby and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question. For example, something like titanic_df.groupby(sex)[\"age\"].describe(), and then titanic_df.groupby(\"sex\")[age].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb6e9d",
   "metadata": {},
   "source": [
    "#### 1.\n",
    "\n",
    "\"df.groupby(\"col1\")[\"col2\"].describe()\" organizes the data set based on the common values in a specified column (that in the round parentheses). It then provides same statistics as the \".describe()\" method for the column specified in the square brackets, each row being the different group that were previously created. \n",
    "\n",
    "For example, if two columns in a data set are the animal's species and the animal's weight, applying \"df.groupby(\"species\")[\"weight\"].describe()\" would first organize all of the animals by their species (ex: cow, pig, sheep), and then provide the .describe() statistics for the weight column of each group's animals.\n",
    "\n",
    "In the examples of my previously used \"housing\" data set and the \"titanic\" data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d9afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  count         mean          std    min      25%     50%  \\\n",
      "ocean_proximity                                                             \n",
      "<1H OCEAN        9136.0  2628.343586  2160.463696   11.0  1464.00  2108.0   \n",
      "INLAND           6551.0  2717.742787  2385.831111    2.0  1404.00  2131.0   \n",
      "ISLAND              5.0  1574.600000   707.545264  716.0   996.00  1675.0   \n",
      "NEAR BAY         2290.0  2493.589520  1830.817022    8.0  1431.25  2083.0   \n",
      "NEAR OCEAN       2658.0  2583.700903  1990.724760   15.0  1505.00  2195.0   \n",
      "\n",
      "                     75%      max  \n",
      "ocean_proximity                    \n",
      "<1H OCEAN        3141.00  37937.0  \n",
      "INLAND           3216.00  39320.0  \n",
      "ISLAND           2127.00   2359.0  \n",
      "NEAR BAY         3029.75  18634.0  \n",
      "NEAR OCEAN       3109.00  30405.0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424.0</td>\n",
       "      <td>30.626179</td>\n",
       "      <td>14.172110</td>\n",
       "      <td>1.00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290.0</td>\n",
       "      <td>28.343690</td>\n",
       "      <td>14.950952</td>\n",
       "      <td>0.42</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count       mean        std   min   25%   50%   75%   max\n",
       "survived                                                           \n",
       "0         424.0  30.626179  14.172110  1.00  21.0  28.0  39.0  74.0\n",
       "1         290.0  28.343690  14.950952  0.42  19.0  28.0  36.0  80.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the housing dataset\n",
    "url1 = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "df1 = pd.read_csv(url1)\n",
    "\n",
    "# Group data by proximity to the ocean and analyze the total number of rooms for each group\n",
    "print(df1.groupby(\"ocean_proximity\")[\"total_rooms\"].describe())\n",
    "\n",
    "# Load the titanic dataset\n",
    "url2 = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df2 = pd.read_csv(url2)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df2.groupby(\"survived\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79151592",
   "metadata": {},
   "source": [
    "### Summary of Interaction\n",
    "\n",
    "Summary of Titanic Dataset Analysis\n",
    "\n",
    "In this interaction, we explored using pandas' groupby and describe functions for analyzing the Titanic dataset.\n",
    "\n",
    "1. df.groupby(\"col1\")[\"col2\"].describe():\n",
    "        Groups data by col1 (e.g., Pclass), then computes summary statistics (count, mean, std, min, max, percentiles) for col2 (e.g., Fare) within each group.\n",
    "\n",
    "    Example: df.groupby(\"Pclass\")[\"Fare\"].describe() shows fare statistics for each passenger class, revealing differences in fare distributions.\n",
    "\n",
    "2. Comparison with df.describe():\n",
    "        df.describe() provides summary statistics for all numerical columns across the entire dataset, without grouping.\n",
    "        df.groupby(\"Pclass\")[\"Fare\"].describe() focuses on a specific column (Fare) and groups by Pclass, offering more detailed insights by class.\n",
    "\n",
    "This analysis highlights how groupby allows for more granular, group-specific insights compared to the overall summary from df.describe().\n",
    "\n",
    "https://chatgpt.com/share/eb7252da-e992-4c5e-9292-1d7672ab7c4a\n",
    "\n",
    "\n",
    "\n",
    "#### 2.\n",
    "\n",
    "The values in the \"count\" column that result from executing \"df.groupby(\"col1\")[\"col2\"].describe()\" provide the number of values that fall under each category that the data has been grouped into. \n",
    "\n",
    "The count column in the description of both the overall data and the specific groups has the function of determining the number of non-missing values. The difference is that counting the values in a sorted data set has the advantage of providing the  distribution of values for of the created groups. \n",
    "\n",
    "In the above example, \"df.groupby(\"ocean_proximity\")[\"total_rooms\"].describe()\" was used on the housing data set. By doing this, the number of values in the \"total_rooms\" can be seen, however the number of values is additionally distributed into each \"ocean_proximity\" category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029f698",
   "metadata": {},
   "source": [
    "#### 3.\n",
    "\n",
    "#### NOTE: The kernel must be restarted to display each of the following sample errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c66090b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Error 1 - Not loading the pandas library\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the titanic dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Group data by whether or not the passenger survived and analyze the total ages of each group\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Error 1 - Not loading the pandas library\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df.groupby(\"survived\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cdc0aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the titanic dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Group data by whether or not the passenger survived and analyze the total ages of each group\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# Error 2 - Typo in the link\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df.groupby(\"survived\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b0eb88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Group data by whether or not the passenger survived and analyze the total ages of each group\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "# Error 3 - Data frame of undefined variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "DF.groupby(\"survived\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baaaf94c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3161805177.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    df.groupby\"survived\")[\"age\"].describe()\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# Error 4 - Forgetting parentheses\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df.groupby\"survived\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827f6b9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'groupbuy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_132/4001825215.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Group data by whether or not the passenger survived and analyze the total ages of each group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupbuy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"survived\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'groupbuy'"
     ]
    }
   ],
   "source": [
    "# Error 5 - Mispelling a function\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df.groupbuy(\"survived\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2c548d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'surviyed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Group data by whether or not the passenger survived and analyze the total ages of each group\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurviyed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'surviyed'"
     ]
    }
   ],
   "source": [
    "# Error 6 - Mispelling column names\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df.groupby(\"surviyed\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4d63f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'survived' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Group data by whether or not the passenger survived and analyze the total ages of each group\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msurvived\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'survived' is not defined"
     ]
    }
   ],
   "source": [
    "# Error 6 - Mispelling column names\n",
    "\n",
    "# Load the titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group data by whether or not the passenger survived and analyze the total ages of each group\n",
    "df.groupby(survived)[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86b798",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "For all provided errors, ChatGPT was able to immediately provide the correct solution to the issue, along with the completed and correct code. This was extremely effective and useful for trouble shooting the errors in the code.\n",
    "\n",
    "Regarding searching for solutions on Google, it was possible to find help for the different errors, however ChatGPT was undeniably more effective and simple to use. One primary advantage of ChatGPT is that it corrected the exact code for my project, as opposed to Google informing me of the general formats that the code should have (after which I would need to apply my data to that format). \n",
    "\n",
    "### Summary of Interactions:\n",
    "\n",
    "In this conversation, the user encountered several errors while working with a dataset from the Titanic. The issues and their resolutions included:\n",
    "\n",
    "    NameError for pd: The user initially received a NameError because the pandas library was not imported with the alias pd. The solution was to add import pandas as pd at the beginning of the script.\n",
    "\n",
    "    HTTPError 404: The user faced a 404 Not Found error due to an incorrect URL for the dataset. The correct URL was provided, which resolved the issue.\n",
    "\n",
    "    Syntax Error in groupby: A SyntaxError occurred because of a missing parenthesis in the groupby method. This was fixed by adding the missing parenthesis.\n",
    "\n",
    "    AttributeError for groupbuy: The user mistyped groupbuy instead of groupby. Correcting the typo resolved the issue.\n",
    "\n",
    "    KeyError for Column Name: A KeyError was raised due to a typo in the column name \"surviyed\" instead of \"survived\". The correct column name was used to fix the error.\n",
    "\n",
    "    NameError for Unquoted Column Name: Finally, the user received a NameError because the column name was used without quotes. The correct usage included quotes around the column name \"survived\".\n",
    "\n",
    "Each issue was addressed by correcting typos or ensuring proper syntax and usage of Python and pandas methods.\n",
    "\n",
    "https://chatgpt.com/share/66e3b6e0-231c-8012-a5ff-41c9712b86ca\n",
    "\n",
    "## 9. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?\n",
    "\n",
    "Yes. ChatGPT and the textbook have helped me to understand the code and concepts we have learned.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
